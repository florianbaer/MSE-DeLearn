{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89825ebe-8b47-4b15-8554-8abaf468f68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflorianbaer\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to your W&B account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d750ee-0749-4beb-9331-0cf4928b77d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.mnist.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.mnist.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2da4ed-af9a-4fc7-b59f-135b51241d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = torch.utils.data.random_split(training_data, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b30c81-d89f-4947-aa09-ab8c70dd262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data),len(validation_data),len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ff404-5b8a-4f63-9730-d6708d2ac8d1",
   "metadata": {},
   "source": [
    "### MLP with Dropout Regularisation\n",
    "\n",
    "Use different dropout rates for the input layer (`p_in`) and hidden layers (`p_hidden`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61b15df-a84b-483d-9365-d8f66390b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_in = 0.2\n",
    "p_hidden = 0.5\n",
    "n_units = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "           Dropout-2                  [-1, 784]               0\n",
      "            Linear-3                  [-1, 200]         157,000\n",
      "           Dropout-4                  [-1, 200]               0\n",
      "           Sigmoid-5                  [-1, 200]               0\n",
      "            Linear-6                   [-1, 10]           2,010\n",
      "================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.61\n",
      "Estimated Total Size (MB): 0.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    nn.Dropout(p_in),\n",
    "    nn.Linear(28*28, n_units),\n",
    "    nn.Dropout(p_hidden),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(n_units, 10)\n",
    ")\n",
    "from torchsummary import summary\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1af84f4-ed00-417b-88c6-2266c99890ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_units, p_in, p_hidden):\n",
    "    return nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        nn.Dropout(p_in),\n",
    "        nn.Linear(28*28, n_units),\n",
    "        nn.Dropout(p_hidden),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_units, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa393a-9f99-48f4-996d-cf8d2a3b8cd5",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Implement the training / evaluation loop\n",
    "\n",
    "Remember and return training / validation cost and accuracy per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "172c91d1-4c9e-413a-bfff-01f51a1e323a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_eval(model, lr, nepochs, nbatch, training_data, validation_data):\n",
    "    \n",
    "    cost_ce = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(training_data, nbatch, shuffle=True)\n",
    "    val_loader = DataLoader(validation_data, nbatch, shuffle=True)\n",
    "    \n",
    "    cost_hist = []\n",
    "    cost_hist_val = []\n",
    "    acc_hist = []\n",
    "    acc_hist_val = []\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        training_cost, correct = 0.0, 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            predictions = model(inputs)\n",
    "            cost = cost_ce(predictions, targets)\n",
    "            training_cost += cost.item()\n",
    "            correct += (predictions.argmax(dim=1) == targets).type(torch.float).sum().item()\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "        training_cost /= len(train_loader)\n",
    "        training_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        validation_cost, correct = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                predictions = model(inputs)\n",
    "                validation_cost += cost_ce(predictions, targets).item()\n",
    "                correct += (torch.argmax(predictions, dim=1) == targets).sum()\n",
    "        validation_cost /= len(val_loader)\n",
    "        validation_acc = correct / len(val_loader.dataset)\n",
    "        wandb.log({\"train_cost\": training_cost, \"train_acc\": training_acc,\"val_cost\": validation_cost, \"val_acc\": validation_acc})\n",
    "        cost_hist.append(training_cost)\n",
    "        acc_hist.append(training_acc)\n",
    "        cost_hist_val.append(validation_cost)\n",
    "        acc_hist_val.append(validation_acc)\n",
    "        print(\"Epoch {:2}: {:.2f}, {:.2f}, {:.2f}, {:.2f}\"\n",
    "              .format(epoch, training_cost, validation_cost, training_acc, validation_acc))\n",
    "        \n",
    "    \n",
    "    return cost_hist, cost_hist_val, acc_hist, acc_hist_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625a52b-a332-4844-8d7d-6998893a2d70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyse Different Settings\n",
    "\n",
    "Start with a baseline model: 200 units in a single hidden layer; batch size 64; properly tuned learning rate, no dropout.\n",
    "\n",
    "Then play with different model complexities and dropout rates and compare them on the basis of the validation set.\n",
    "\n",
    "Estimate also the variance error by the difference between validation and training loss / accuracy.\n",
    "\n",
    "Finally, identify a favourite combination (model complexity, dropout rate) and compute the test accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1fa1c2b-50d7-4b23-91c8-c47eec5fc8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## complexity: 100, dropout rate: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/MSE-DeLearn/SW04/wandb/run-20220326_115328-1c3qmlu1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/florianbaer/model_selection_FashionMNIST/runs/1c3qmlu1\" target=\"_blank\">experiment_model_selection_1</a></strong> to <a href=\"https://wandb.ai/florianbaer/model_selection_FashionMNIST\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: 0.75, 0.52, 0.74, 0.82\n",
      "Epoch  1: 0.49, 0.55, 0.82, 0.79\n",
      "Epoch  2: 0.44, 0.46, 0.84, 0.82\n",
      "Epoch  3: 0.41, 0.49, 0.85, 0.81\n",
      "Epoch  4: 0.40, 0.41, 0.86, 0.85\n",
      "Epoch  5: 0.38, 0.61, 0.86, 0.76\n",
      "Epoch  6: 0.37, 0.40, 0.87, 0.85\n",
      "Epoch  7: 0.36, 0.37, 0.87, 0.86\n",
      "Epoch  8: 0.35, 0.38, 0.87, 0.87\n",
      "Epoch  9: 0.34, 0.50, 0.88, 0.80\n",
      "Epoch 10: 0.33, 0.38, 0.88, 0.86\n",
      "Epoch 11: 0.33, 0.37, 0.88, 0.86\n",
      "Epoch 12: 0.32, 0.42, 0.88, 0.84\n",
      "Epoch 13: 0.31, 0.41, 0.89, 0.85\n",
      "Epoch 14: 0.31, 0.34, 0.89, 0.88\n",
      "Epoch 15: 0.30, 0.37, 0.89, 0.87\n",
      "Epoch 16: 0.30, 0.35, 0.89, 0.87\n",
      "Epoch 17: 0.29, 0.43, 0.89, 0.83\n",
      "Epoch 18: 0.29, 0.41, 0.90, 0.85\n",
      "Epoch 19: 0.28, 0.34, 0.90, 0.88\n",
      "Epoch 20: 0.28, 0.33, 0.90, 0.88\n",
      "Epoch 21: 0.28, 0.36, 0.90, 0.87\n",
      "Epoch 22: 0.27, 0.38, 0.90, 0.86\n",
      "Epoch 23: 0.27, 0.34, 0.90, 0.88\n",
      "Epoch 24: 0.26, 0.35, 0.90, 0.87\n",
      "Epoch 25: 0.26, 0.32, 0.90, 0.88\n",
      "Epoch 26: 0.26, 0.40, 0.91, 0.86\n",
      "Epoch 27: 0.26, 0.38, 0.91, 0.87\n",
      "Epoch 28: 0.25, 0.35, 0.91, 0.87\n",
      "Epoch 29: 0.25, 0.34, 0.91, 0.87\n",
      "Epoch 30: 0.24, 0.32, 0.91, 0.88\n",
      "Epoch 31: 0.24, 0.31, 0.91, 0.89\n",
      "Epoch 32: 0.24, 0.31, 0.91, 0.89\n",
      "Epoch 33: 0.23, 0.31, 0.91, 0.89\n",
      "Epoch 34: 0.23, 0.33, 0.91, 0.88\n",
      "Epoch 35: 0.23, 0.37, 0.92, 0.87\n",
      "Epoch 36: 0.23, 0.34, 0.92, 0.88\n",
      "Epoch 37: 0.22, 0.33, 0.92, 0.88\n",
      "Epoch 38: 0.22, 0.32, 0.92, 0.89\n",
      "Epoch 39: 0.22, 0.31, 0.92, 0.89\n",
      "Epoch 40: 0.22, 0.36, 0.92, 0.87\n",
      "Epoch 41: 0.21, 0.36, 0.92, 0.87\n",
      "Epoch 42: 0.21, 0.36, 0.92, 0.87\n",
      "Epoch 43: 0.21, 0.31, 0.92, 0.89\n",
      "Epoch 44: 0.21, 0.37, 0.92, 0.87\n",
      "Epoch 45: 0.21, 0.50, 0.93, 0.82\n",
      "Epoch 46: 0.20, 0.32, 0.93, 0.89\n",
      "Epoch 47: 0.20, 0.38, 0.93, 0.87\n",
      "Epoch 48: 0.20, 0.34, 0.93, 0.88\n",
      "Epoch 49: 0.20, 0.32, 0.93, 0.89\n",
      "Epoch 50: 0.19, 0.36, 0.93, 0.87\n",
      "Epoch 51: 0.19, 0.33, 0.93, 0.88\n",
      "Epoch 52: 0.19, 0.32, 0.93, 0.89\n",
      "Epoch 53: 0.19, 0.34, 0.93, 0.88\n",
      "Epoch 54: 0.19, 0.33, 0.93, 0.89\n",
      "Epoch 55: 0.18, 0.37, 0.93, 0.87\n",
      "Epoch 56: 0.18, 0.33, 0.93, 0.89\n",
      "Epoch 57: 0.18, 0.33, 0.94, 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "costs = {\"train\":[], \"valid\":[]}\n",
    "accs =  {\"train\":[], \"valid\":[]}\n",
    "nbatch = 64\n",
    "nepochs = 100\n",
    "lr = 0.25\n",
    "\n",
    "complexity = [100, 300, 500]\n",
    "drop_p = [0, 0.2, 0.3]\n",
    "\n",
    "hists = [costs[\"train\"], costs[\"valid\"], accs[\"train\"], accs[\"valid\"]]\n",
    "i = 1\n",
    "for c in complexity:\n",
    "    for p in drop_p:\n",
    "        print(f\"######## complexity: {c}, dropout rate: {p}\")\n",
    "        wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"model_selection_FashionMNIST\", \n",
    "            # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "            name=f\"experiment_model_selection_{i}\", \n",
    "            # Track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"dataset\": \"FashionMNIST\",\n",
    "            \"epochs\": nepochs,\n",
    "            \"p_in\":p,\n",
    "            \"p_hidden\":p,\n",
    "            \"hidden_units\": c   \n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model = get_model(n_units=c, p_in=p, p_hidden=p)\n",
    "        res = train_eval(model, lr, nepochs, nbatch, training_data, validation_data)\n",
    "        \n",
    "        [hist.append(val) for (hist, val) in zip(hists, res)]\n",
    "        wandb.finish()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b7ada-69f8-4531-9dc7-06cf0573284c",
   "metadata": {},
   "source": [
    "### Suitable Output Plots\n",
    "\n",
    "Possibly adjust to fit your needs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9077b7-8b2b-429b-b5c4-c024f1daadf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process wandb_internal:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/internal/internal.py\", line 162, in wandb_internal\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 1053, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 1069, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/spawn.py\", line 129, in _main\n",
      "    return self._bootstrap(parent_sentinel)\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 333, in _bootstrap\n",
      "    threading._shutdown()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 1448, in _shutdown\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt\n",
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 167, in check_status\n",
      "    status_response = self._interface.communicate_stop_status()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\", line 114, in communicate_stop_status\n",
      "    resp = self._communicate_stop_status(status)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 387, in _communicate_stop_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 222, in _communicate\n",
      "    Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 227, in _communicate_async\n",
      "        raise Exception(\"The wandb backend process has shutdown\")self.run()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 149, in check_network_status\n",
      "    status_response = self._interface.communicate_network_status()\n",
      "Exception: The wandb backend process has shutdown\n",
      "\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\", line 125, in communicate_network_status\n",
      "    resp = self._communicate_network_status(status)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 397, in _communicate_network_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 222, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 227, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    }
   ],
   "source": [
    "plt.figure(1, figsize=(12,8))\n",
    "for ic, c in enumerate(complexity):\n",
    "    for ip, p in enumerate(drop_p):\n",
    "        i = ip + ic * len(complexity)\n",
    "        plt.plot(torch.arange(nepochs), costs[\"train\"][i], color=f\"C{i}\", linestyle=\"--\", label=f\"train {p},{c}\")\n",
    "        plt.plot(torch.arange(nepochs), costs[\"valid\"][i], color=f\"C{i}\", label=f\"valid {p},{c}\")\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.xlim(0, nepochs)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Cross-Entropy Cost\", fontsize=18)\n",
    "plt.legend()\n",
    "plt.figure(2, figsize=(12,8))\n",
    "for ic, c in enumerate(complexity):\n",
    "    for ip, p in enumerate(drop_p):\n",
    "        i = ip + ic * len(complexity)\n",
    "        plt.plot(torch.arange(nepochs), accs[\"train\"][i], color=f\"C{i}\", linestyle=\"--\", label=f\"train {p},{c}\")\n",
    "        plt.plot(torch.arange(nepochs), accs[\"valid\"][i], color=f\"C{i}\", label=f\"valid {p},{c}\")\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.xlim(0, nepochs)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.title(\"Accuracy\", fontsize=18)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0bef3-7aeb-4329-aa64-e8d107c9bee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153ab65-021e-4bba-8d09-038b2fe5134c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
