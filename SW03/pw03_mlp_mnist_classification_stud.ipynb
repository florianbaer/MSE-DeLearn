{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Load train and test partition of the MNIST dataset.\n",
    "\n",
    "Prepare the training by splitting the training partition into a training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2461a8e-1930-4ed6-b6cc-dabf6dc8057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(training_data)*0.8)\n",
    "test_size = len(training_data)-training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20431bb-1ece-49f5-a5f6-414a48dcc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into train and validate\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "### YOUR CODE START ###\n",
    "\n",
    "training_set, validation_set = random_split(training_data,[training_size,test_size])\n",
    "\n",
    "### YOUR CODE END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ff404-5b8a-4f63-9730-d6708d2ac8d1",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Implement an MLP model that can be configured with a an arbitrary number of layers and units per layer.\n",
    "\n",
    "To that end, implement a suitable sub-class of `torch.nn.Module` with a constructor that accepts the following arguments:\n",
    "* `units`: list of integers that specify the number of units in the different layers. The first element corresponds to the number of units in the input layer (layer '0'), the last element is the number of output units, i.e. the number of classes the classifier is designed for (10 for an MNIST classifier). Hence, MLP will have $n$ hidden layers if `units` has $n+1$ elements. \n",
    "* `activation_class`: Class name of the activation function layer to be used (such as `torch.nn.ReLU`). Instances can be created by `activation_class()` and added to the succession of layers defined by the model. \n",
    "\n",
    "Alternatively, you can implement a utility method that creates a `torch.nn.Sequential` model accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e55861c-e424-45b4-845a-48dd576d7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE START ###\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, units, activation_class = None):\n",
    "        self.layers = []\n",
    "        super(MLP,self).__init__()\n",
    "        self.nlayers = len(units)\n",
    "        #self.layers.append(torch.nn.Flatten()) \n",
    "        self.layernames = []\n",
    "        self.__setattr__('flatten_1', torch.nn.Flatten())    ##### <==== see here\n",
    "        self.layernames.append('flatten_1')\n",
    "        idx = 2\n",
    "        for i in range(self.nlayers-2):\n",
    "            \n",
    "            self.__setattr__(f'linear_{idx}', torch.nn.Linear(units[i], units[i+1]))\n",
    "            self.layernames.append(f'linear_{idx}')\n",
    "            \n",
    "            if activation_class is None:\n",
    "                self.__setattr__(f'actv_{idx+1}', torch.nn.ReLU())\n",
    "                self.layernames.append(f'actv_{idx+1}')\n",
    "            else:\n",
    "                self.__setattr__(f'actv_{idx+1}', activation_class())\n",
    "                self.layernames.append(f'actv_{idx+1}')\n",
    "            idx += 2\n",
    "        self.__setattr__(f'linear_{idx}', torch.nn.Linear(units[len(units)-2], units[len(units)-1]))\n",
    "        self.layernames.append(f'linear_{idx}')\n",
    "        #print([layer for layer in self.layers])\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = x.clone()\n",
    "        \n",
    "        #z = self.flatten(x) ##### <==== see here\n",
    "        for layer in self.layernames:\n",
    "            \n",
    "            z = self.__getattr__(layer)(z)\n",
    "        return z  \n",
    "\n",
    "### YOUR CODE END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 300]         235,500\n",
      "              ReLU-3                  [-1, 300]               0\n",
      "            Linear-4                  [-1, 100]          30,100\n",
      "              ReLU-5                  [-1, 100]               0\n",
      "            Linear-6                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.02\n",
      "Estimated Total Size (MB): 1.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28,300, 100, 10])\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff38adb-7cad-4eee-b6ed-34c0e0fd3d07",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "For training, implement a method with the arguments:\n",
    "* `model`: Model to be trained\n",
    "* `lr`: Learning rate\n",
    "* `nepochs`: Number of epochs\n",
    "* `batchsize`: Batch size\n",
    "* `training_data`: Training set (subclassed of `Dataset`)\n",
    "* `validation_data`: Validation set (subclassed of `Dataset`)\n",
    "\n",
    "Remember the training and validation cost and accuracy, respectively for monitoring the progress of the training. <br>\n",
    "Note that for the training cost and accuracy you can use the per batch quantities averaged over an epoch. \n",
    "\n",
    "Furthermore, you can use the SGD optimizer of pytorch (`torch.optim.SGD`) - but without momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "172c91d1-4c9e-413a-bfff-01f51a1e323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, lr, nepochs, nbatch, training_set, validation_set):\n",
    "    # finally return the sequence of per epoch values\n",
    "    cost_hist = []\n",
    "    cost_hist_valid = []\n",
    "    acc_hist = []\n",
    "    acc_hist_valid = []\n",
    "\n",
    "    cost_ce = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    ### YOUR CODE START ###\n",
    "    \n",
    "    # epoch: current epoch\n",
    "    # cost, cost_valid, acc, acc_valid: cost and acurracy (for training, validation set) per epoch     \n",
    "    \n",
    "    training_loader = DataLoader(training_set, batch_size=nbatch)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=nbatch)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "\n",
    "        training_cost = 0\n",
    "        correct = 0\n",
    "        for inputs, targets in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs)\n",
    "            cost = cost_ce(predictions, targets)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "            training_cost += cost.item()\n",
    "            correct += (torch.argmax(predictions, dim=1) == targets).sum()\n",
    "        \n",
    "        cost = training_cost / len(training_set)\n",
    "        acc = correct / len(training_set)\n",
    "\n",
    "        validation_cost = 0\n",
    "        correct = 0\n",
    "        for inputs, targets in validation_loader:\n",
    "            predictions = model(inputs)\n",
    "            cost = cost_ce(predictions, targets)\n",
    "            validation_cost += cost.item()\n",
    "            correct += (torch.argmax(predictions, dim=1) == targets).sum()\n",
    "\n",
    "        cost_valid = validation_cost / len(validation_set)\n",
    "        acc_valid = correct / len(validation_set)\n",
    "        \n",
    "        print(\"Epoch %i: %f, %f, %f, %f\"%(epoch, cost, acc, cost_valid, acc_valid))\n",
    "\n",
    "        ### YOUR CODE END ###\n",
    "        \n",
    "        cost_hist.append(cost.data)\n",
    "        cost_hist_valid.append(cost_valid)\n",
    "        acc_hist.append(acc)\n",
    "        acc_hist_valid.append(acc_valid)\n",
    "    return cost_hist, cost_hist_valid, acc_hist, acc_hist_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b84285-bbd4-4a65-8b8c-1a535f680fff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration\n",
    "\n",
    "Now use this functionality to explore different layer configurations: \n",
    "* Number of layers\n",
    "* Number of units per layer\n",
    "* Suitable learning rate\n",
    "* Suitable number of epochs.\n",
    "\n",
    "Use a batchsize of 64.\n",
    "\n",
    "Make sure that you choose a sufficinetly large number of epochs so that the learning has more or less stabilizes (converged). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed706360-8df4-4ca4-ba49-952534a5e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.526094, 0.387750, 0.025471, 0.580917\n",
      "Epoch 1: 0.909135, 0.707979, 0.014619, 0.774083\n",
      "Epoch 2: 0.701220, 0.810479, 0.010538, 0.822167\n",
      "Epoch 3: 0.623750, 0.844771, 0.008847, 0.844500\n",
      "Epoch 4: 0.584618, 0.863708, 0.007906, 0.859083\n",
      "Epoch 5: 0.562121, 0.874875, 0.007297, 0.870333\n",
      "Epoch 6: 0.547863, 0.883125, 0.006871, 0.878167\n",
      "Epoch 7: 0.537547, 0.888625, 0.006561, 0.880917\n",
      "Epoch 8: 0.529595, 0.892042, 0.006325, 0.884250\n",
      "Epoch 9: 0.522818, 0.894917, 0.006139, 0.886500\n",
      "Epoch 10: 0.516079, 0.897396, 0.005987, 0.888667\n",
      "Epoch 11: 0.509194, 0.899562, 0.005860, 0.890833\n",
      "Epoch 12: 0.502777, 0.901208, 0.005752, 0.892750\n",
      "Epoch 13: 0.496686, 0.902667, 0.005657, 0.895000\n",
      "Epoch 14: 0.491055, 0.904479, 0.005573, 0.896833\n",
      "Epoch 15: 0.485565, 0.905854, 0.005497, 0.897917\n",
      "Epoch 16: 0.480343, 0.907292, 0.005429, 0.899333\n",
      "Epoch 17: 0.475610, 0.908708, 0.005365, 0.901083\n",
      "Epoch 18: 0.471775, 0.909917, 0.005307, 0.902333\n",
      "Epoch 19: 0.469319, 0.911000, 0.005253, 0.903417\n",
      "train_accuracy 0.9110000133514404, val_accuracy 0.9034166932106018\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28,10, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.005, 20, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c5c8fc-6929-435a-bf83-3e3d80654709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.470590, 0.800750, 0.005415, 0.900083\n",
      "Epoch 1: 0.394060, 0.916792, 0.004164, 0.923417\n",
      "Epoch 2: 0.316332, 0.935542, 0.003347, 0.938000\n",
      "Epoch 3: 0.248632, 0.948625, 0.002797, 0.948250\n",
      "Epoch 4: 0.204454, 0.957417, 0.002412, 0.955500\n",
      "Epoch 5: 0.171525, 0.964104, 0.002131, 0.960000\n",
      "Epoch 6: 0.152941, 0.969333, 0.001927, 0.962500\n",
      "Epoch 7: 0.138211, 0.973521, 0.001776, 0.965667\n",
      "Epoch 8: 0.131266, 0.977146, 0.001660, 0.968167\n",
      "Epoch 9: 0.126039, 0.979708, 0.001568, 0.969667\n",
      "Epoch 10: 0.122767, 0.982542, 0.001494, 0.971833\n",
      "Epoch 11: 0.117698, 0.984625, 0.001438, 0.972583\n",
      "Epoch 12: 0.112344, 0.986875, 0.001394, 0.973250\n",
      "Epoch 13: 0.104434, 0.988500, 0.001359, 0.973750\n",
      "Epoch 14: 0.096911, 0.990062, 0.001328, 0.974500\n",
      "Epoch 15: 0.092030, 0.991500, 0.001310, 0.974833\n",
      "Epoch 16: 0.084524, 0.992521, 0.001292, 0.975333\n",
      "Epoch 17: 0.076341, 0.993813, 0.001283, 0.975750\n",
      "Epoch 18: 0.069496, 0.995000, 0.001277, 0.976083\n",
      "Epoch 19: 0.062835, 0.995729, 0.001270, 0.976667\n",
      "Epoch 20: 0.058253, 0.996708, 0.001271, 0.976500\n",
      "Epoch 21: 0.052568, 0.997146, 0.001276, 0.976583\n",
      "Epoch 22: 0.049381, 0.997813, 0.001275, 0.977000\n",
      "Epoch 23: 0.045017, 0.998396, 0.001280, 0.977000\n",
      "Epoch 24: 0.042464, 0.998625, 0.001286, 0.976917\n",
      "Epoch 25: 0.039541, 0.998812, 0.001286, 0.977000\n",
      "Epoch 26: 0.038683, 0.999125, 0.001288, 0.977500\n",
      "Epoch 27: 0.036241, 0.999250, 0.001291, 0.977667\n",
      "Epoch 28: 0.033943, 0.999354, 0.001292, 0.977583\n",
      "Epoch 29: 0.032713, 0.999500, 0.001298, 0.977500\n",
      "train_accuracy 0.9994999766349792, val_accuracy 0.9775000214576721\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28, 300, 100, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.05, 30, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058feb6e-fd8d-4949-9a88-0ee928886e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.443315, 0.687000, 0.005704, 0.893667\n",
      "Epoch 1: 0.339967, 0.914875, 0.003787, 0.929417\n",
      "Epoch 2: 0.244934, 0.943313, 0.002775, 0.946750\n",
      "Epoch 3: 0.177933, 0.957750, 0.002232, 0.957917\n",
      "Epoch 4: 0.127428, 0.966500, 0.001889, 0.964417\n",
      "Epoch 5: 0.099255, 0.973750, 0.001662, 0.968333\n",
      "Epoch 6: 0.078899, 0.979021, 0.001514, 0.972417\n",
      "Epoch 7: 0.066396, 0.983167, 0.001433, 0.973417\n",
      "Epoch 8: 0.056571, 0.986479, 0.001391, 0.974583\n",
      "Epoch 9: 0.047236, 0.989021, 0.001368, 0.975167\n",
      "Epoch 10: 0.040297, 0.991354, 0.001369, 0.975333\n",
      "Epoch 11: 0.037941, 0.993688, 0.001363, 0.976000\n",
      "Epoch 12: 0.034309, 0.995354, 0.001372, 0.976250\n",
      "Epoch 13: 0.029889, 0.996458, 0.001391, 0.976833\n",
      "Epoch 14: 0.027230, 0.997667, 0.001403, 0.976750\n",
      "Epoch 15: 0.022056, 0.998354, 0.001438, 0.976417\n",
      "Epoch 16: 0.019677, 0.998833, 0.001466, 0.976750\n",
      "Epoch 17: 0.017375, 0.999229, 0.001488, 0.976667\n",
      "Epoch 18: 0.013451, 0.999500, 0.001476, 0.976667\n",
      "Epoch 19: 0.013004, 0.999646, 0.001412, 0.977333\n",
      "Epoch 20: 0.013151, 0.999792, 0.001432, 0.977333\n",
      "Epoch 21: 0.014250, 0.999854, 0.001452, 0.977250\n",
      "Epoch 22: 0.013640, 0.999917, 0.001471, 0.977167\n",
      "Epoch 23: 0.013860, 0.999958, 0.001490, 0.977333\n",
      "Epoch 24: 0.013509, 0.999958, 0.001503, 0.977083\n",
      "Epoch 25: 0.012839, 0.999979, 0.001516, 0.977250\n",
      "Epoch 26: 0.012103, 0.999979, 0.001529, 0.977167\n",
      "Epoch 27: 0.011683, 0.999979, 0.001540, 0.977500\n",
      "Epoch 28: 0.011263, 0.999979, 0.001550, 0.977667\n",
      "Epoch 29: 0.010551, 0.999979, 0.001561, 0.977917\n",
      "train_accuracy 0.9999791383743286, val_accuracy 0.9779166579246521\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28, 500,300, 100, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.05, 30, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceed8d92-d270-4a00-9a93-bda166b2d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.438900, 0.821875, 0.005144, 0.905750\n",
      "Epoch 1: 0.338543, 0.919875, 0.004014, 0.926750\n",
      "Epoch 2: 0.272688, 0.937542, 0.003295, 0.940083\n",
      "Epoch 3: 0.221854, 0.948417, 0.002807, 0.948583\n",
      "Epoch 4: 0.185421, 0.956979, 0.002464, 0.953917\n",
      "Epoch 5: 0.158095, 0.962625, 0.002214, 0.957917\n",
      "Epoch 6: 0.130408, 0.967875, 0.002001, 0.962083\n",
      "Epoch 7: 0.119372, 0.971583, 0.001848, 0.964250\n",
      "Epoch 8: 0.107013, 0.974688, 0.001717, 0.966333\n",
      "Epoch 9: 0.099006, 0.977771, 0.001628, 0.967750\n",
      "train_accuracy 0.9777708053588867, val_accuracy 0.9677500128746033\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28, 100, 500, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.05, 10, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03c1dd2-ca40-40af-96f3-ad703c20074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.485014, 0.783646, 0.005698, 0.894750\n",
      "Epoch 1: 0.377636, 0.911729, 0.004482, 0.917250\n",
      "Epoch 2: 0.306156, 0.929021, 0.003731, 0.930500\n",
      "Epoch 3: 0.241460, 0.942500, 0.003191, 0.940750\n",
      "Epoch 4: 0.194938, 0.951604, 0.002789, 0.948083\n",
      "Epoch 5: 0.172424, 0.958229, 0.002500, 0.952333\n",
      "Epoch 6: 0.154281, 0.963208, 0.002283, 0.956833\n",
      "Epoch 7: 0.143050, 0.967188, 0.002103, 0.960917\n",
      "Epoch 8: 0.130611, 0.971000, 0.001957, 0.963667\n",
      "Epoch 9: 0.125209, 0.973875, 0.001861, 0.965250\n",
      "train_accuracy 0.9738749861717224, val_accuracy 0.9652500152587891\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28, 100, 50, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.05, 10, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c19f6-694f-4447-8ca3-79ef2f8daade",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Summarize your findings with the different settings in a table\n",
    "\n",
    "| Units | nepochs | lr | Acc (Train) | Acc (Valid) |\n",
    "| --- | :-: | :-: | :-: | :-: |\n",
    "| (784,10,10) | 20 | 0.5 | 91.1% | 90.3% |\n",
    "| (784,300,100,10) | 10 | 0.05 | 99.9% | 97.7% |\n",
    "| (784,500,300,100,10) | 30 | 0.005 | 99.9% | 97.8% |\n",
    "| (784,100,500,10) | 10 | 0.05 | 97.8% | 96.7% |\n",
    "| (784,100,50,10) | 10 | 0.05 | 97.38% | 96.5% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fbbd92-7623-451f-8eac-92f8243f9f16",
   "metadata": {},
   "source": [
    "I prever the Model with the following attributes `(784,100,500,10)\t10\t0.05\t97.8%\t96.7%`\n",
    "\n",
    "It does not seem to have such an immense overfit as the last one in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00728601-d9cf-41af-9e38-cabae9097698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
