{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ec37d7-f7cb-4646-a2dd-9dab66239a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8a986-1cb2-483b-9bd8-0f71d69fa9e8",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Load train and test partition of the MNIST dataset.\n",
    "\n",
    "Prepare the training by splitting the training partition into a training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ba6b0c6-3f55-40dd-ba32-6f6aad8f1437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.mnist.MNIST'>\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2461a8e-1930-4ed6-b6cc-dabf6dc8057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(training_data)*0.8)\n",
    "test_size = len(training_data)-training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e20431bb-1ece-49f5-a5f6-414a48dcc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into train and validate\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "### YOUR CODE START ###\n",
    "\n",
    "training_set, validation_set = random_split(training_data,[training_size,test_size])\n",
    "\n",
    "### YOUR CODE END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ff404-5b8a-4f63-9730-d6708d2ac8d1",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Implement an MLP model that can be configured with a an arbitrary number of layers and units per layer.\n",
    "\n",
    "To that end, implement a suitable sub-class of `torch.nn.Module` with a constructor that accepts the following arguments:\n",
    "* `units`: list of integers that specify the number of units in the different layers. The first element corresponds to the number of units in the input layer (layer '0'), the last element is the number of output units, i.e. the number of classes the classifier is designed for (10 for an MNIST classifier). Hence, MLP will have $n$ hidden layers if `units` has $n+1$ elements. \n",
    "* `activation_class`: Class name of the activation function layer to be used (such as `torch.nn.ReLU`). Instances can be created by `activation_class()` and added to the succession of layers defined by the model. \n",
    "\n",
    "Alternatively, you can implement a utility method that creates a `torch.nn.Sequential` model accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e55861c-e424-45b4-845a-48dd576d7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE START ###\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, units, activation_class = None):\n",
    "        super(MLP,self).__init__()\n",
    "        self.flatten_1 = torch.nn.Flatten()\n",
    "        self.linear_2  = torch.nn.Linear(units[0],units[1])\n",
    "        self.relu_3  =  torch.nn.ReLU()\n",
    "        self.linear_4  = torch.nn.Linear(units[1],units[2])\n",
    "        self.relu_5    = torch.nn.ReLU()\n",
    "        self.linear_6  = torch.nn.Linear(units[2],units[3])\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.linear_2(self.flatten_1(x))\n",
    "        z = self.relu_3(z)\n",
    "        z = self.linear_4(z)\n",
    "        z = self.relu_5(z)\n",
    "        z = self.linear_6(z)\n",
    "        return z\n",
    "        \n",
    "\n",
    "### YOUR CODE END ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4bfd0a4-999c-4690-9ef3-b5ee1ab26c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 300]         235,500\n",
      "              ReLU-3                  [-1, 300]               0\n",
      "            Linear-4                  [-1, 100]          30,100\n",
      "              ReLU-5                  [-1, 100]               0\n",
      "            Linear-6                   [-1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 1.02\n",
      "Estimated Total Size (MB): 1.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28,300, 100, 10])\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff38adb-7cad-4eee-b6ed-34c0e0fd3d07",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "For training, implement a method with the arguments:\n",
    "* `model`: Model to be trained\n",
    "* `lr`: Learning rate\n",
    "* `nepochs`: Number of epochs\n",
    "* `batchsize`: Batch size\n",
    "* `training_data`: Training set (subclassed of `Dataset`)\n",
    "* `validation_data`: Validation set (subclassed of `Dataset`)\n",
    "\n",
    "Remember the training and validation cost and accuracy, respectively for monitoring the progress of the training. <br>\n",
    "Note that for the training cost and accuracy you can use the per batch quantities averaged over an epoch. \n",
    "\n",
    "Furthermore, you can use the SGD optimizer of pytorch (`torch.optim.SGD`) - but without momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "172c91d1-4c9e-413a-bfff-01f51a1e323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, lr, nepochs, nbatch, training_set, validation_set):\n",
    "    # finally return the sequence of per epoch values\n",
    "    cost_hist = []\n",
    "    cost_hist_valid = []\n",
    "    acc_hist = []\n",
    "    acc_hist_valid = []\n",
    "\n",
    "    cost_ce = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    ### YOUR CODE START ###\n",
    "    \n",
    "    # epoch: current epoch\n",
    "    # cost, cost_valid, acc, acc_valid: cost and acurracy (for training, validation set) per epoch     \n",
    "    \n",
    "    training_loader = DataLoader(training_data, batch_size=nbatch, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=10000, shuffle=True)\n",
    "    Xtest, Ytest = next(iter(test_loader))\n",
    "    size = len(training_loader.dataset)\n",
    "    nbatches = len(training_loader)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        cost, acc = 0.0, 0.0\n",
    "        for batch, (X, Y) in enumerate(training_loader):\n",
    "            pred = model(X)\n",
    "            cost = cost_ce(pred, Y)\n",
    "            acc += (pred.argmax(dim=1) == Y).type(torch.float).sum().item()\n",
    "\n",
    "            # gradient, parameter update\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "        # cost /= nbatches\n",
    "        acc /= size\n",
    "\n",
    "        cost_valid, acc_valid = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, Y, in test_loader:\n",
    "                pred = model(X)\n",
    "                cost_valid = cost_ce(pred, Y)\n",
    "                acc_valid = (pred.argmax(dim=1) == Y).type(torch.float).sum().item()\n",
    "        cost_valid /= len(test_loader)\n",
    "        acc_valid /= len(test_loader.dataset)\n",
    "        \n",
    "        print(\"Epoch %i: %f, %f, %f, %f\"%(epoch, cost, acc, cost_valid, acc_valid))\n",
    "\n",
    "        ### YOUR CODE END ###\n",
    "        \n",
    "        cost_hist.append(cost)\n",
    "        cost_hist_valid.append(cost_valid)\n",
    "        acc_hist.append(acc)\n",
    "        acc_hist_valid.append(acc_valid)\n",
    "    return cost_hist, cost_hist_valid, acc_hist, acc_hist_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b84285-bbd4-4a65-8b8c-1a535f680fff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration\n",
    "\n",
    "Now use this functionality to explore different layer configurations: \n",
    "* Number of layers\n",
    "* Number of units per layer\n",
    "* Suitable learning rate\n",
    "* Suitable number of epochs.\n",
    "\n",
    "Use a batchsize of 64.\n",
    "\n",
    "Make sure that you choose a sufficinetly large number of epochs so that the learning has more or less stabilizes (converged). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57c344e2-003c-43d4-a8eb-0e26ac028b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP4L(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, units, activation_class = None):\n",
    "        super(MLP4L,self).__init__()\n",
    "        self.flatten_1 = torch.nn.Flatten()\n",
    "        self.linear_2  = torch.nn.Linear(units[0],units[1])\n",
    "        self.relu_3  =  torch.nn.ReLU()\n",
    "        self.linear_4  = torch.nn.Linear(units[1],units[2])\n",
    "        self.relu_5    = torch.nn.ReLU()\n",
    "        self.linear_6  = torch.nn.Linear(units[2],units[3])\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.linear_2(self.flatten_1(x))\n",
    "        z = self.relu_3(z)\n",
    "        z = self.linear_4(z)\n",
    "        z = self.relu_5(z)\n",
    "        z = self.linear_6(z)\n",
    "        return z\n",
    "\n",
    "class MLP5L(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, units, activation_class = None):\n",
    "        super(MLP5L,self).__init__()\n",
    "        self.flatten_1 = torch.nn.Flatten()\n",
    "        self.linear_2  = torch.nn.Linear(units[0],units[1])\n",
    "        self.relu_3  =  torch.nn.ReLU()\n",
    "        self.linear_4  = torch.nn.Linear(units[1],units[2])\n",
    "        self.relu_5    = torch.nn.ReLU()\n",
    "        self.linear_6  = torch.nn.Linear(units[2],units[3])\n",
    "        self.relu_7    = torch.nn.ReLU()\n",
    "        self.linear_8  = torch.nn.Linear(units[3],units[4])\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.linear_2(self.flatten_1(x))\n",
    "        z = self.relu_3(z)\n",
    "        z = self.linear_4(z)\n",
    "        z = self.relu_5(z)\n",
    "        z = self.linear_6(z)\n",
    "        z = self.relu_7(z)\n",
    "        z = self.linear_8(z)\n",
    "        return z\n",
    "    \n",
    "class MLP6L(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, units, activation_class = None):\n",
    "        super(MLP5L,self).__init__()\n",
    "        self.flatten_1 = torch.nn.Flatten()\n",
    "        self.linear_2  = torch.nn.Linear(units[0],units[1])\n",
    "        self.relu_3  =  torch.nn.ReLU()\n",
    "        self.linear_4  = torch.nn.Linear(units[1],units[2])\n",
    "        self.relu_5    = torch.nn.ReLU()\n",
    "        self.linear_6  = torch.nn.Linear(units[2],units[3])\n",
    "        self.relu_7    = torch.nn.ReLU()\n",
    "        self.linear_8  = torch.nn.Linear(units[3],units[4])\n",
    "        self.relu_9    = torch.nn.ReLU()\n",
    "        self.linear_10  = torch.nn.Linear(units[5],units[6])\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.linear_2(self.flatten_1(x))\n",
    "        z = self.relu_3(z)\n",
    "        z = self.linear_4(z)\n",
    "        z = self.relu_5(z)\n",
    "        z = self.linear_6(z)\n",
    "        z = self.relu_7(z)\n",
    "        z = self.linear_8(z)\n",
    "        z = self.relu_9(z)\n",
    "        z = self.linear_10(z)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed706360-8df4-4ca4-ba49-952534a5e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.230736, 0.154817, 2.244276, 0.365500\n",
      "Epoch 1: 1.730013, 0.418333, 1.702673, 0.529200\n",
      "Epoch 2: 0.696280, 0.690167, 0.770192, 0.774600\n",
      "Epoch 3: 0.457589, 0.805833, 0.549864, 0.838400\n",
      "Epoch 4: 0.627504, 0.855833, 0.444192, 0.870800\n",
      "Epoch 5: 0.205761, 0.879017, 0.387338, 0.887200\n",
      "Epoch 6: 0.187060, 0.891567, 0.354102, 0.898000\n",
      "Epoch 7: 0.143832, 0.900133, 0.331479, 0.905400\n",
      "Epoch 8: 0.309057, 0.907000, 0.310427, 0.909300\n",
      "Epoch 9: 0.214746, 0.912583, 0.289715, 0.915900\n",
      "Epoch 10: 0.336021, 0.916933, 0.278952, 0.920600\n",
      "Epoch 11: 0.193686, 0.921517, 0.261938, 0.925600\n",
      "Epoch 12: 0.222441, 0.926100, 0.249639, 0.927500\n",
      "Epoch 13: 0.315549, 0.929817, 0.241067, 0.931900\n",
      "Epoch 14: 0.295807, 0.932917, 0.224346, 0.936900\n",
      "Epoch 15: 0.472278, 0.936783, 0.216438, 0.939700\n",
      "Epoch 16: 0.159291, 0.939267, 0.204230, 0.941000\n",
      "Epoch 17: 0.246632, 0.941917, 0.195720, 0.944400\n",
      "Epoch 18: 0.094231, 0.944433, 0.188206, 0.945800\n",
      "Epoch 19: 0.149558, 0.946900, 0.182838, 0.947900\n",
      "Epoch 20: 0.111168, 0.948733, 0.174380, 0.949200\n",
      "Epoch 21: 0.348155, 0.951417, 0.171847, 0.949900\n",
      "Epoch 22: 0.109015, 0.953183, 0.170177, 0.950700\n",
      "Epoch 23: 0.118622, 0.954717, 0.157751, 0.952900\n",
      "Epoch 24: 0.057241, 0.956483, 0.152171, 0.955100\n",
      "Epoch 25: 0.090851, 0.958183, 0.147956, 0.956200\n",
      "Epoch 26: 0.050207, 0.959267, 0.142465, 0.956600\n",
      "Epoch 27: 0.032264, 0.960983, 0.139604, 0.958200\n",
      "Epoch 28: 0.238364, 0.962250, 0.134926, 0.959300\n",
      "Epoch 29: 0.177891, 0.964217, 0.134820, 0.960000\n",
      "train_accuracy 0.9642166666666667, val_accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "model = MLP5L([28*28,500, 300, 100, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.005, 30, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3c5c8fc-6929-435a-bf83-3e3d80654709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.306316, 0.813167, 0.315312, 0.908500\n",
      "Epoch 1: 0.251741, 0.924400, 0.216339, 0.936000\n",
      "Epoch 2: 0.267334, 0.943233, 0.164234, 0.952900\n",
      "Epoch 3: 0.223379, 0.955700, 0.141268, 0.956500\n",
      "Epoch 4: 0.095921, 0.963833, 0.128064, 0.960700\n",
      "Epoch 5: 0.012733, 0.969700, 0.100661, 0.968400\n",
      "Epoch 6: 0.019854, 0.974033, 0.097006, 0.969900\n",
      "Epoch 7: 0.019475, 0.977250, 0.085023, 0.972800\n",
      "Epoch 8: 0.014026, 0.980133, 0.086449, 0.972100\n",
      "Epoch 9: 0.009115, 0.982600, 0.077764, 0.976100\n",
      "Epoch 10: 0.017065, 0.985350, 0.076842, 0.975800\n",
      "Epoch 11: 0.100301, 0.986983, 0.077165, 0.975000\n",
      "Epoch 12: 0.024994, 0.988683, 0.078208, 0.975200\n",
      "Epoch 13: 0.014860, 0.990200, 0.067405, 0.977900\n",
      "Epoch 14: 0.022580, 0.991133, 0.068705, 0.977700\n",
      "Epoch 15: 0.012145, 0.992267, 0.067183, 0.978600\n",
      "Epoch 16: 0.007576, 0.993433, 0.071132, 0.977500\n",
      "Epoch 17: 0.006002, 0.994517, 0.064572, 0.979500\n",
      "Epoch 18: 0.008288, 0.995300, 0.065683, 0.978900\n",
      "Epoch 19: 0.025417, 0.996500, 0.067301, 0.979500\n",
      "Epoch 20: 0.035615, 0.997033, 0.065309, 0.980300\n",
      "Epoch 21: 0.001694, 0.997550, 0.066818, 0.979200\n",
      "Epoch 22: 0.045021, 0.997783, 0.068967, 0.980300\n",
      "Epoch 23: 0.007404, 0.998267, 0.066366, 0.980100\n",
      "Epoch 24: 0.009285, 0.998767, 0.065553, 0.980400\n",
      "Epoch 25: 0.000225, 0.999067, 0.067243, 0.981200\n",
      "Epoch 26: 0.016373, 0.999133, 0.069621, 0.980600\n",
      "Epoch 27: 0.005236, 0.999350, 0.067700, 0.980600\n",
      "Epoch 28: 0.002615, 0.999467, 0.067664, 0.980600\n",
      "Epoch 29: 0.005692, 0.999467, 0.068431, 0.980100\n",
      "train_accuracy 0.9994666666666666, val_accuracy 0.9801\n"
     ]
    }
   ],
   "source": [
    "model = MLP4L([28*28, 300, 100, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.05, 30, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "058feb6e-fd8d-4949-9a88-0ee928886e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.561184, 0.810183, 0.312940, 0.907100\n",
      "Epoch 1: 0.226070, 0.918017, 0.258294, 0.925500\n",
      "Epoch 2: 0.233998, 0.940733, 0.191774, 0.941800\n",
      "Epoch 3: 0.183037, 0.952767, 0.145208, 0.955900\n",
      "Epoch 4: 0.137461, 0.960767, 0.128336, 0.961200\n",
      "Epoch 5: 0.225403, 0.966750, 0.124678, 0.962300\n",
      "Epoch 6: 0.179621, 0.971100, 0.115188, 0.966200\n",
      "Epoch 7: 0.056146, 0.974567, 0.099261, 0.969600\n",
      "Epoch 8: 0.054897, 0.977217, 0.096771, 0.970400\n",
      "Epoch 9: 0.109749, 0.979283, 0.099586, 0.969700\n",
      "Epoch 10: 0.017977, 0.982067, 0.086884, 0.973000\n",
      "Epoch 11: 0.021430, 0.983450, 0.084277, 0.973200\n",
      "Epoch 12: 0.093491, 0.985517, 0.101964, 0.969200\n",
      "Epoch 13: 0.005897, 0.985967, 0.083285, 0.975200\n",
      "Epoch 14: 0.008281, 0.988033, 0.079412, 0.975900\n",
      "Epoch 15: 0.010568, 0.989633, 0.080077, 0.975800\n",
      "Epoch 16: 0.028055, 0.990183, 0.081462, 0.974100\n",
      "Epoch 17: 0.007607, 0.990917, 0.076026, 0.976100\n",
      "Epoch 18: 0.051711, 0.992217, 0.077234, 0.977000\n",
      "Epoch 19: 0.004848, 0.993017, 0.076359, 0.977900\n",
      "Epoch 20: 0.003397, 0.993683, 0.079060, 0.976600\n",
      "Epoch 21: 0.016661, 0.994150, 0.076040, 0.977300\n",
      "Epoch 22: 0.038792, 0.994933, 0.076290, 0.978000\n",
      "Epoch 23: 0.000685, 0.995933, 0.077082, 0.977400\n",
      "Epoch 24: 0.014058, 0.996217, 0.076238, 0.978000\n",
      "Epoch 25: 0.029614, 0.996683, 0.078177, 0.978100\n",
      "Epoch 26: 0.002425, 0.997050, 0.076462, 0.977200\n",
      "Epoch 27: 0.002619, 0.997667, 0.083126, 0.976400\n",
      "Epoch 28: 0.007133, 0.998083, 0.077653, 0.977300\n",
      "Epoch 29: 0.001937, 0.998400, 0.079889, 0.977700\n",
      "train_accuracy 0.9984, val_accuracy 0.9777\n"
     ]
    }
   ],
   "source": [
    "model = MLP4L([28*28, 100, 50, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.05, 30, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ceed8d92-d270-4a00-9a93-bda166b2d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.003397, 0.998700, 0.079191, 0.978200\n",
      "Epoch 1: 0.020406, 0.998867, 0.081839, 0.977600\n",
      "Epoch 2: 0.001967, 0.998950, 0.079048, 0.978300\n",
      "Epoch 3: 0.008159, 0.998967, 0.084851, 0.977500\n",
      "Epoch 4: 0.002401, 0.999350, 0.080948, 0.978200\n",
      "Epoch 5: 0.000453, 0.999433, 0.081796, 0.978500\n",
      "Epoch 6: 0.004543, 0.999567, 0.082780, 0.977900\n",
      "Epoch 7: 0.001239, 0.999650, 0.082206, 0.978600\n",
      "Epoch 8: 0.000582, 0.999733, 0.083712, 0.978300\n",
      "Epoch 9: 0.000614, 0.999767, 0.083512, 0.978400\n",
      "train_accuracy 0.9997666666666667, val_accuracy 0.9784\n"
     ]
    }
   ],
   "source": [
    "model_final = MLP4L([28*28, 100, 500, 10])\n",
    "_,_, acc_hist, acc_val_hist = train_eval(model, 0.05, 10, 64, training_set, validation_set)\n",
    "print(f'train_accuracy {acc_hist[-1]}, val_accuracy {acc_val_hist[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c19f6-694f-4447-8ca3-79ef2f8daade",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Summarize your findings with the different settings in a table\n",
    "\n",
    "| Units | nepochs | lr | Acc (Train) | Acc (Valid) |\n",
    "| --- | :-: | :-: | :-: | :-: |\n",
    "| (784,10,10) | 20 | 0.5 | 94.1% | 93.4% |\n",
    "| (784,300,100,10) | 10 | 0.05 | 98.2% | 97.4% |\n",
    "| (784,500,300,100,10) | 30 | 0.005 | 96.4% | 96.0% |\n",
    "| (784,100,500,10) | 10 | 0.05 | 98.0% | 97.3% |\n",
    "| (784,100,50,10) | 10 | 0.05 | 99.84% | 97.7% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fbbd92-7623-451f-8eac-92f8243f9f16",
   "metadata": {},
   "source": [
    "I prever the Model with the following attributes `(784,100,500,10)\t10\t0.05\t98.0%\t97.3%`\n",
    "\n",
    "It does not seem to have such an immense overfit as the last one in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c799245-9dfd-4959-853a-d9d1febd1772",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_final\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m Y \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mtargets\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36mMLP4L.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu_3(z)\n\u001b[1;32m     16\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_4(z)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/flatten.py:42\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_dim)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataset.py:83\u001b[0m, in \u001b[0;36mDataset.__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_final.eval()\n",
    "pred = model_final(test_data)\n",
    "Y = test_data.targets\n",
    "print(Y)\n",
    "#acc_valid = (pred.argmax(dim=1) == Y).type(torch.float).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e31d0-ca62-41cc-a027-1c9963426d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
